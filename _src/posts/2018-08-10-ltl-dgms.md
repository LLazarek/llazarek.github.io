    Title: Understanding the evaluation of LTL formulas with diagrams
    Date: 2018-08-10T14:48:54
    Tags: rv, theory, DRAFT

<!-- more -->

I'm currently spending a lot of time working with LTL formulas.
Specifically, I'm trying to understand how to evaluate them on finite prefixes of infinite sequences.
Though there are many resources online that explain how LTL formulas work on sequences, some with nice diagrams, I didn't find anything that pictorally demonstrates how LTL formulas are evaluated on infinite sequences.
So I drew these diagrams up to help my own understanding.

A little explanation: in the diagrams, states are shown as blue circles.
Above each state is an LTL formula (which in the most basic case is just a predicate) which that state must satisfy in order for the overall formula to be satisfied (I call them *requirements* for the states).
Requirements in blue are just "pre-conditions"; they must be satisfied for the overall formula to be determined true, but their being satisfied cannot determine if the overall formula is satisfied.
Requirements in red are "determining" in the sense that their satisfaction determines the overall formula's satisfaction.

For example, consider the diagram for `X p`.
The first state has a pre-condition of `any`, so any kind of state will satisfy it.
The next state has a determining condition of `p`, so if the next state satisfies `p`, the whole sequence satisfies the formula; otherwise, the whole sequence fails.

Basic LTL formula combinators:
<br></br>
![img](/img/ltl-dgms-1-small.png)

Thinking about these formulas operationally &#x2013; in terms of checking them against a sequence that occurs one state at a time &#x2013; provides an some interesting perspective, though.
Returning to `X p`, for example; operationally, it "takes" (up to) two states to verify whether or not `X p` is satisfied for a given sequence.
That's because we need to see the first state, and verify that it (trivially) satisfies `any`.
Then we need to see the second state, and verify that it satisfies `p`.

From this operational perspective, consider the *until* operator (**U**).
All of the resources I could find explain how it works quite simply (as I did above) when the sub-formulas of the **U** take a single state to check.
Just like in the above diagram, every state not satisfying `B` must satisfy `A`, until some state satisfies `B`.
So for every state that comes in we must check if `B` is satisfied, and if not `B` then check that `A` is satisfied.
Simple enough?

`A` and `B` are supposed to represent arbitrary LTL formulas, though.
What if `A` is a formula that takes, say, three states to check?
And what if `B` takes a few states to check, too?
It's easy enough to come up with an example, such as `(p \land X X p) U (X p)`.

From a declarative, theoretical perspective the answer is easy (relatively): just non-deterministically select at which state the `B` formula will become true, and the entire formula can be accepted upon the *next* state by checking that all subsequences starting at every state before the previous state satisfy `A`.
From an operational perspective, this means continually "spinning up threads" to check both `A` and `B` as if every new state was the start of a sequence.


The below diagram depicts this non-determinism from the operational perspective.
Each `B`-thread can only accept dependent upon the acceptance of every `A`-thread of a smaller index than the `B`-thread.

For example, if the `B`-thread with index 3 accepts, then A threads with index 1 and 2 must accept for the formula to be satisfied.

Non-determinism in checking **U** for multi-step sub-formulas:
<br></br>
![img](/img/ltl-dgms-2-small.png)
